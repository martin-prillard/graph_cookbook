{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centrality Measures in Social Networks ðŸ“±\n",
    "\n",
    "Welcome to this comprehensive tutorial on applying centrality measures to real social network data! In this notebook, we'll explore how graph theory can be used to analyze social networks and identify influential users.\n",
    "\n",
    "## Concepts Covered:\n",
    "- **Loading Real Social Network Data**: https://github.com/eleurent/twitter-graph\n",
    "- **Centrality Measures**: Degree, Betweenness, Eigenvector, PageRank, Closeness Centrality\n",
    "- **Influential User Identification**: Finding key users in social networks\n",
    "- **Community Structure**: Understanding network topology\n",
    "- **Real-World Applications**: Marketing, information spread, viral content analysis\n",
    "\n",
    "## What You'll Learn:\n",
    "1. How to load and analyze real social network datasets\n",
    "2. How to compute various centrality measures on large networks\n",
    "3. How to identify influential users and opinion leaders\n",
    "4. How different centrality measures reveal different types of influence\n",
    "5. Visualizing and interpreting centrality results in social networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import requests\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Loading Real Social Network Data\n",
    "\n",
    "We'll load a real social network from the **friends.gephi** file. This is a real social network graph exported from Twitter, containing friendship connections with many nodes and edges.\n",
    "\n",
    "See: https://github.com/eleurent/twitter-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_graphml('data/friends.graphml')\n",
    "G = G.to_undirected()\n",
    "\n",
    "# only for testing:\n",
    "#n_sub = min(1000, G.number_of_nodes())\n",
    "#sample_nodes = random.sample(list(G.nodes()), n_sub)\n",
    "#G = G.subgraph(sample_nodes).copy()\n",
    "\n",
    "# Create a mapping from node ID to display name\n",
    "# Check for common attribute names that might contain profile names\n",
    "def get_display_name(node_id, node_attrs):\n",
    "    \"\"\"Extract display name from node attributes, fallback to node_id.\"\"\"\n",
    "    # Common attribute names for profile names\n",
    "    name_attrs = ['label', 'Label', 'name', 'Name', 'screen_name', 'screenName', \n",
    "                  'username', 'userName', 'display_name', 'displayName', 'id_str']\n",
    "    \n",
    "    for attr in name_attrs:\n",
    "        if attr in node_attrs and node_attrs[attr]:\n",
    "            name = str(node_attrs[attr]).strip()\n",
    "            if name and name != node_id:  # Only use if it's different from ID\n",
    "                return name\n",
    "    \n",
    "    # Fallback to node_id\n",
    "    return str(node_id)\n",
    "\n",
    "# Build name mapping\n",
    "node_to_name = {}\n",
    "for node_id in G.nodes():\n",
    "    node_attrs = G.nodes[node_id]\n",
    "    node_to_name[node_id] = get_display_name(node_id, node_attrs)\n",
    "\n",
    "# Check what attributes are available\n",
    "if G.nodes():\n",
    "    sample_node = list(G.nodes())[0]\n",
    "    sample_attrs = G.nodes[sample_node]\n",
    "    print(f\"\\nðŸ“‹ Sample node attributes: {list(sample_attrs.keys())}\")\n",
    "    print(f\"   Sample node ID: {sample_node}\")\n",
    "    print(f\"   Sample display name: {node_to_name[sample_node]}\")\n",
    "\n",
    "print(f\"\\nâœ… Social network loaded!\")\n",
    "print(f\"   Nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"   Edges: {G.number_of_edges():,}\")\n",
    "\n",
    "print(f\"   Average degree: {2 * G.number_of_edges() / G.number_of_nodes():.2f}\")\n",
    "print(f\"   Density: {nx.density(G):.6f}\")\n",
    "print(f\"   Connected: {nx.is_connected(G)}\")\n",
    "\n",
    "if not nx.is_connected(G):\n",
    "    components = list(nx.connected_components(G))\n",
    "    print(f\"   Components: {len(components)}\")\n",
    "    largest_cc = max(components, key=len)\n",
    "    G = G.subgraph(largest_cc).copy()\n",
    "    # Update name mapping for subgraph\n",
    "    node_to_name = {node: node_to_name.get(node, str(node)) for node in G.nodes()}\n",
    "    print(f\"   Using largest component: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Network Statistics and Basic Analysis\n",
    "\n",
    "Let's explore the basic properties of our social network to understand its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute basic network statistics\n",
    "print(\"ðŸ“Š NETWORK STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Degree distribution\n",
    "degrees = [G.degree(n) for n in G.nodes()]\n",
    "degree_series = pd.Series(degrees)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Degree Distribution:\")\n",
    "print(f\"   Mean degree: {degree_series.mean():.2f}\")\n",
    "print(f\"   Median degree: {degree_series.median():.2f}\")\n",
    "print(f\"   Max degree: {degree_series.max()}\")\n",
    "print(f\"   Min degree: {degree_series.min()}\")\n",
    "print(f\"   Std deviation: {degree_series.std():.2f}\")\n",
    "\n",
    "# Network metrics\n",
    "print(f\"\\nðŸŒ Network Topology:\")\n",
    "print(f\"   Average clustering coefficient: {nx.average_clustering(G):.4f}\")\n",
    "print(f\"   Transitivity: {nx.transitivity(G):.4f}\")\n",
    "#print(f\"   Diameter: {nx.diameter(G)}\")\n",
    "print(f\"   Average path length: {nx.average_shortest_path_length(G):.4f}\")\n",
    "\n",
    "# Power-law check (real social networks often follow power-law)\n",
    "print(f\"\\nðŸ“‰ Network Properties:\")\n",
    "print(f\"   Is scale-free (power-law): Checking degree distribution...\")\n",
    "# Check if degree distribution follows power-law (many low-degree, few high-degree)\n",
    "high_degree_nodes = sum(1 for d in degrees if d > degree_series.quantile(0.9))\n",
    "print(f\"   High-degree nodes (>90th percentile): {high_degree_nodes} ({100*high_degree_nodes/len(degrees):.2f}%)\")\n",
    "\n",
    "# Visualize degree distribution\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Degree Distribution (Linear)', 'Degree Distribution (Log-Log)',\n",
    "                   'Network Overview', 'Top 20 Most Connected Users'),\n",
    "    specs=[[{\"type\": \"histogram\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# Linear degree distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=degrees, nbinsx=50, name='Degree Distribution',\n",
    "                marker_color='#4ECDC4'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Log-log degree distribution (to check power-law)\n",
    "degree_counts = pd.Series(degrees).value_counts().sort_index()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=degree_counts.index, y=degree_counts.values,\n",
    "              mode='markers', name='Degree Distribution (Log-Log)',\n",
    "              marker=dict(size=8, color='#FF6B6B')),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Network overview stats\n",
    "stats = {\n",
    "    'Nodes': G.number_of_nodes(),\n",
    "    'Edges': G.number_of_edges(),\n",
    "    'Avg Degree': f\"{degree_series.mean():.1f}\",\n",
    "    'Density': f\"{nx.density(G):.6f}\"\n",
    "}\n",
    "# For bar chart, we'll show normalized values\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(stats.keys())[:2], y=[G.number_of_nodes(), G.number_of_edges()],\n",
    "           name='Count', marker_color='#95E1D3', text=[f\"{v:,}\" for v in [G.number_of_nodes(), G.number_of_edges()]],\n",
    "           textposition='outside'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Top 20 most connected users\n",
    "top_degrees = sorted([(n, G.degree(n)) for n in G.nodes()], key=lambda x: x[1], reverse=True)[:20]\n",
    "top_nodes = [n for n, d in top_degrees]\n",
    "top_degree_values = [d for n, d in top_degrees]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=top_nodes, y=top_degree_values, name='Top Users by Degree',\n",
    "           marker_color='#F38181', text=top_degree_values, textposition='outside'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(title_text=\"Degree\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Degree (log)\", type=\"log\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Frequency (log)\", type=\"log\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Metric\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"User\", tickangle=-45, row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Number of Connections\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    showlegend=False,\n",
    "    title_text=\"Social Network Overview and Statistics\",\n",
    "    title_x=0.5\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nâœ… Network analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Computing Centrality Measures\n",
    "\n",
    "Now we'll compute various centrality measures to identify influential users in the social network. Each measure reveals different aspects of influence:\n",
    "\n",
    "- **Degree Centrality**: Users with the most connections (popularity)\n",
    "- **Betweenness Centrality**: Users who act as bridges between different groups\n",
    "- **Closeness Centrality**: Users who can reach others quickly\n",
    "- **Eigenvector Centrality**: Users connected to other important users\n",
    "- **PageRank**: Google's algorithm adapted for social networks\n",
    "- **Katz Centrality**: Measures influence by considering both direct and indirect connections with distance-based attenuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all centrality measures\n",
    "print(\"ðŸ” Computing centrality measures...\")\n",
    "print(\"   This may take a few moments for large networks...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 1. Degree Centrality (fastest)\n",
    "print(\"   1. Computing Degree Centrality...\")\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "degree_time = time.time() - start_time\n",
    "print(f\"      âœ“ Completed in {degree_time:.2f} seconds\")\n",
    "\n",
    "# 2. Betweenness Centrality (can be slow for large networks)\n",
    "print(\"   2. Computing Betweenness Centrality...\")\n",
    "btw_start = time.time()\n",
    "# For very large networks, we can use approximation\n",
    "if G.number_of_nodes() > 5000:\n",
    "    print(\"      Using approximation (k=100) for faster computation...\")\n",
    "    betweenness_centrality = nx.betweenness_centrality(G, k=100, seed=42)\n",
    "else:\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "btw_time = time.time() - btw_start\n",
    "print(f\"      âœ“ Completed in {btw_time:.2f} seconds\")\n",
    "\n",
    "# 3. Closeness Centrality\n",
    "print(\"   3. Computing Closeness Centrality...\")\n",
    "close_start = time.time()\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "close_time = time.time() - close_start\n",
    "print(f\"      âœ“ Completed in {close_time:.2f} seconds\")\n",
    "\n",
    "# 4. Eigenvector Centrality\n",
    "print(\"   4. Computing Eigenvector Centrality...\")\n",
    "eigen_start = time.time()\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000, tol=1e-06)\n",
    "eigen_time = time.time() - eigen_start\n",
    "print(f\"      âœ“ Completed in {eigen_time:.2f} seconds\")\n",
    "\n",
    "# 5. PageRank\n",
    "print(\"   5. Computing PageRank...\")\n",
    "pagerank_start = time.time()\n",
    "pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)\n",
    "pagerank_time = time.time() - pagerank_start\n",
    "print(f\"      âœ“ Completed in {pagerank_time:.2f} seconds\")\n",
    "\n",
    "# 6. Katz Centrality\n",
    "print(\"   6. Computing Katz Centrality...\")\n",
    "katz_start = time.time()\n",
    "# Katz centrality with attenuation factor (alpha)\n",
    "# alpha should be less than 1/largest_eigenvalue for convergence\n",
    "# We'll use a safe default value\n",
    "katz_centrality = nx.katz_centrality(G, alpha=0.005, max_iter=10000, tol=1e-05)\n",
    "katz_time = time.time() - katz_start\n",
    "print(f\"      âœ“ Completed in {katz_time:.2f} seconds\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nâœ… All 6 centrality measures computed in {total_time:.2f} seconds!\")\n",
    "\n",
    "# Create comprehensive DataFrame\n",
    "centrality_df = pd.DataFrame({\n",
    "    'user': list(G.nodes()),\n",
    "    'degree': [degree_centrality.get(n, 0) for n in G.nodes()],\n",
    "    'degree_count': [G.degree(n) for n in G.nodes()],\n",
    "    'betweenness': [betweenness_centrality.get(n, 0) for n in G.nodes()],\n",
    "    'closeness': [closeness_centrality.get(n, 0) for n in G.nodes()],\n",
    "    'eigenvector': [eigenvector_centrality.get(n, 0) for n in G.nodes()],\n",
    "    'pagerank': [pagerank.get(n, 0) for n in G.nodes()],\n",
    "    'katz': [katz_centrality.get(n, 0) for n in G.nodes()]\n",
    "})\n",
    "\n",
    "# Add display names for better readability\n",
    "centrality_df['display_name'] = centrality_df['user'].map(lambda x: node_to_name.get(x, x))\n",
    "\n",
    "print(f\"\\nðŸ“Š Centrality Measures Summary:\")\n",
    "print(f\"   Total users analyzed: {len(centrality_df):,}\")\n",
    "print(f\"\\n   Centrality Score Ranges:\")\n",
    "for col in ['degree', 'betweenness', 'closeness', 'eigenvector', 'pagerank', 'katz']:\n",
    "    print(f\"      {col.capitalize():15s}: [{centrality_df[col].min():.6f}, {centrality_df[col].max():.6f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Top Influential Users by Each Centrality Measure\n",
    "\n",
    "Let's identify the most influential users according to each centrality measure. Different measures reveal different types of influence!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top users by each centrality measure\n",
    "top_n = 15\n",
    "\n",
    "print(\"ðŸŒŸ TOP INFLUENTIAL USERS BY CENTRALITY MEASURE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "measures = {\n",
    "    'degree': 'Degree Centrality (Most Connections)',\n",
    "    'betweenness': 'Betweenness Centrality (Network Bridges)',\n",
    "    'closeness': 'Closeness Centrality (Fastest Reach)',\n",
    "    'eigenvector': 'Eigenvector Centrality (Connected to Important Users)',\n",
    "    'pagerank': 'PageRank (Overall Influence)',\n",
    "    'katz': 'Katz Centrality (Direct + Indirect Influence)',\n",
    "}\n",
    "\n",
    "top_users_by_measure = {}\n",
    "\n",
    "for measure, description in measures.items():\n",
    "    top_users = centrality_df.nlargest(top_n, measure)\n",
    "    top_users_by_measure[measure] = top_users\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {description}:\")\n",
    "    print(\"-\" * 80)\n",
    "    for idx, (i, row) in enumerate(top_users.iterrows(), 1):\n",
    "        print(f\"   {idx:2d}. {row['display_name']:20s} | Score: {row[measure]:.6f} | Connections: {row['degree_count']}\")\n",
    "    \n",
    "    # Show overlap with other measures\n",
    "    top_users_set = set(top_users['user'])\n",
    "    overlaps = {}\n",
    "    for other_measure, other_description in measures.items():\n",
    "        if other_measure != measure:\n",
    "            other_top = set(centrality_df.nlargest(top_n, other_measure)['user'])\n",
    "            overlap = len(top_users_set & other_top)\n",
    "            overlaps[other_measure] = overlap\n",
    "    \n",
    "    print(f\"   Overlap with other top-{top_n} lists:\")\n",
    "    for other_measure, overlap in overlaps.items():\n",
    "        print(f\"      - {measures[other_measure]}: {overlap}/{top_n} users\")\n",
    "\n",
    "# Create visualization\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=list(measures.values()),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#95E1D3', '#F38181', '#AA96DA', '#FCBAD3']\n",
    "row_col = [(1,1), (1,2), (2,1), (2,2), (3,1), (3,2)]\n",
    "\n",
    "for idx, (measure, description) in enumerate(measures.items()):\n",
    "    top_users = top_users_by_measure[measure]\n",
    "    row, col = row_col[idx]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=top_users['display_name'].map(lambda x: node_to_name.get(x, x)), y=top_users[measure],\n",
    "               name=description, marker_color=colors[idx],\n",
    "               text=top_users[measure].round(4),\n",
    "               textposition='outside',\n",
    "               hovertemplate='<b>%{x}</b><br>Score: %{y:.6f}<br>Connections: %{customdata}<extra></extra>',\n",
    "               customdata=top_users['degree_count']),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "for i in range(1, 4):\n",
    "    for j in range(1, 3):\n",
    "        fig.update_xaxes(tickangle=-45, row=i, col=j)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1400,\n",
    "    showlegend=False,\n",
    "    title_text=\"Top 15 Influential Users by Different Centrality Measures\",\n",
    "    title_x=0.5\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Comparing Centrality Measures\n",
    "\n",
    "Different centrality measures capture different aspects of influence. Let's compare them to understand:\n",
    "- Which users are consistently influential across all measures?\n",
    "- Which users are influential in specific ways?\n",
    "- How do the measures correlate with each other?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between centrality measures\n",
    "print(\"ðŸ”— CORRELATION BETWEEN CENTRALITY MEASURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "correlation_matrix = centrality_df[['degree', 'betweenness', 'closeness', 'eigenvector', 'pagerank', 'katz']].corr()\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Find users who are consistently in top-N across multiple measures\n",
    "print(f\"\\nðŸŒŸ USERS CONSISTENTLY INFLUENTIAL ACROSS MULTIPLE MEASURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count how many top-20 lists each user appears in\n",
    "user_scores = defaultdict(int)\n",
    "for measure in ['degree', 'betweenness', 'closeness', 'eigenvector', 'pagerank', 'katz']:\n",
    "    top_20 = set(centrality_df.nlargest(20, measure)['user'])\n",
    "    for user in top_20:\n",
    "        user_scores[user] += 1\n",
    "\n",
    "# Users appearing in multiple top lists\n",
    "multi_measure_users = {user: count for user, count in user_scores.items() if count >= 3}\n",
    "sorted_multi = sorted(multi_measure_users.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nUsers appearing in 3+ top-20 lists ({len(sorted_multi)} users):\")\n",
    "for user, count in sorted_multi[:20]:\n",
    "    user_row = centrality_df[centrality_df['user'] == user].iloc[0]\n",
    "    display_name = user_row['display_name']\n",
    "    print(f\"\\n   {display_name} (ID: {user})\")\n",
    "    print(f\"      Appears in {count}/6 top-20 lists\")\n",
    "    print(f\"      Degree: {user_row['degree_count']}, Betweenness: {user_row['betweenness']:.6f}\")\n",
    "    print(f\"      PageRank: {user_row['pagerank']:.6f}, Eigenvector: {user_row['eigenvector']:.6f}\")\n",
    "\n",
    "# Create comprehensive comparison visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Correlation Heatmap', 'Degree vs Betweenness',\n",
    "                   'Eigenvector vs PageRank', 'Multi-Measure Influence Score'),\n",
    "    specs=[[{\"type\": \"heatmap\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# Correlation heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=correlation_matrix.values,\n",
    "        x=correlation_matrix.columns,\n",
    "        y=correlation_matrix.columns,\n",
    "        colorscale='Viridis',\n",
    "        text=correlation_matrix.values.round(3),\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 10},\n",
    "        colorbar=dict(title=\"Correlation\", x=1.02)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Degree vs Betweenness scatter\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centrality_df['degree'],\n",
    "        y=centrality_df['betweenness'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=centrality_df['pagerank'],\n",
    "            colorscale='Plasma',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"PageRank\", x=1.15, y=0.75, len=0.3),\n",
    "            line=dict(width=0.5, color='white')\n",
    "        ),\n",
    "        text=centrality_df['user'].map(lambda x: node_to_name.get(x, x)),\n",
    "        hovertemplate='<b>%{text}</b><br>Degree: %{x:.4f}<br>Betweenness: %{y:.6f}<br>PageRank: %{marker.color:.6f}<extra></extra>',\n",
    "        name='Users'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Eigenvector vs PageRank scatter\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=centrality_df['eigenvector'],\n",
    "        y=centrality_df['pagerank'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=centrality_df['degree_count'],\n",
    "            colorscale='Turbo',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Degree Count\", x=1.15, y=0.25, len=0.3),\n",
    "            line=dict(width=0.5, color='white')\n",
    "        ),\n",
    "        text=centrality_df['user'].map(lambda x: node_to_name.get(x, x)),\n",
    "        hovertemplate='<b>%{text}</b><br>Eigenvector: %{x:.6f}<br>PageRank: %{y:.6f}<br>Connections: %{marker.color}<extra></extra>',\n",
    "        name='Users'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Multi-measure influence score\n",
    "# Create a composite score\n",
    "centrality_df['composite_score'] = (\n",
    "    centrality_df['degree'] * 0.15 +\n",
    "    centrality_df['betweenness'] * 0.15 +\n",
    "    centrality_df['closeness'] * 0.15 +\n",
    "    centrality_df['eigenvector'] * 0.15 +\n",
    "    centrality_df['pagerank'] * 0.20 +\n",
    "    centrality_df['katz'] * 0.20\n",
    ")\n",
    "\n",
    "top_composite = centrality_df.nlargest(20, 'composite_score')\n",
    "fig.add_trace(\n",
    "    go.Bar(x=top_composite['display_name'].map(lambda x: node_to_name.get(x, x)), y=top_composite['composite_score'],\n",
    "           marker_color='#FF6B6B', text=top_composite['composite_score'].round(4),\n",
    "           textposition='outside',\n",
    "           hovertemplate='<b>%{x}</b><br>Composite Score: %{y:.6f}<extra></extra>'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(title_text=\"Centrality Measure\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Centrality Measure\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Degree Centrality\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Betweenness Centrality\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Eigenvector Centrality\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"PageRank\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"User\", tickangle=-45, row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Composite Influence Score\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1000,\n",
    "    width=1300,\n",
    "    showlegend=False,\n",
    "    title_text=\"Centrality Measures Comparison and Analysis\",\n",
    "    title_x=0.5\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Insights and Applications\n",
    "\n",
    "Now let's extract meaningful insights from our centrality analysis and explore real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Identifying Different Types of Influencers\n",
    "\n",
    "In social networks, different users have different types of influence:\n",
    "\n",
    "1. **Celebrities (High Degree)**: Users with many direct connections\n",
    "2. **Bridge Users (High Betweenness)**: Users connecting different communities\n",
    "3. **Opinion Leaders (High Eigenvector)**: Users connected to other influential users\n",
    "4. **Information Spreaders (High Closeness)**: Users who can reach others quickly\n",
    "5. **Overall Influencers (High PageRank)**: Users with balanced influence across measures\n",
    "\n",
    "Centrality measures have many practical applications in social networks:\n",
    "\n",
    "1. **Marketing & Advertising**: Target influential users for product promotion\n",
    "2. **Information Spread**: Identify users who can spread information quickly\n",
    "3. **Viral Content**: Find users likely to make content go viral\n",
    "4. **Community Detection**: Use bridge users to understand community structure\n",
    "5. **Influence Maximization**: Select seed users for maximum network coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application scenarios\n",
    "print(\"ðŸ’¼ REAL-WORLD APPLICATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Marketing: Best users to target for product promotion\n",
    "print(\"\\n1. ðŸ“¢ MARKETING CAMPAIGN TARGETING\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   Strategy: Target users with high PageRank (overall influence)\")\n",
    "print(\"   These users have balanced influence and can reach diverse audiences\\n\")\n",
    "marketing_targets = centrality_df.nlargest(10, 'pagerank')\n",
    "for idx, (i, row) in enumerate(marketing_targets.iterrows(), 1):\n",
    "    print(f\"   {idx:2d}. {row['display_name']:20s} | PageRank: {row['pagerank']:.6f} | Reach: {row['degree_count']} users\")\n",
    "\n",
    "# 2. Information Spread: Users who can spread news quickly\n",
    "print(\"\\n2. ðŸ“° INFORMATION SPREAD (News/Viral Content)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   Strategy: Target users with high Closeness Centrality\")\n",
    "print(\"   These users can reach the entire network quickly\\n\")\n",
    "info_spreaders = centrality_df.nlargest(10, 'closeness')\n",
    "for idx, (i, row) in enumerate(info_spreaders.iterrows(), 1):\n",
    "    print(f\"   {idx:2d}. {row['display_name']:20s} | Closeness: {row['closeness']:.6f} | Avg Distance: {1/row['closeness']:.2f}\")\n",
    "\n",
    "# 3. Community Bridge: Users connecting different groups\n",
    "print(\"\\n3. ðŸŒ‰ COMMUNITY BRIDGE USERS\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   Strategy: Identify users with high Betweenness Centrality\")\n",
    "print(\"   These users connect different communities and can facilitate cross-group communication\\n\")\n",
    "bridge_users = centrality_df.nlargest(10, 'betweenness')\n",
    "for idx, (i, row) in enumerate(bridge_users.iterrows(), 1):\n",
    "    print(f\"   {idx:2d}. {row['display_name']:20s} | Betweenness: {row['betweenness']:.6f} | Connections: {row['degree_count']}\")\n",
    "\n",
    "# 4. Opinion Leaders: Users connected to other influencers\n",
    "print(\"\\n4. ðŸ‘‘ OPINION LEADERS\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   Strategy: Target users with high Eigenvector Centrality\")\n",
    "print(\"   These users are connected to other influential users (prestige effect)\\n\")\n",
    "opinion_leaders = centrality_df.nlargest(10, 'eigenvector')\n",
    "for idx, (i, row) in enumerate(opinion_leaders.iterrows(), 1):\n",
    "    print(f\"   {idx:2d}. {row['display_name']:20s} | Eigenvector: {row['eigenvector']:.6f} | Connections: {row['degree_count']}\")\n",
    "\n",
    "# 5. Influence Maximization: Best seed set for maximum coverage\n",
    "print(\"\\n5. ðŸŽ¯ INFLUENCE MAXIMIZATION (Seed Selection)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"   Strategy: Select diverse set of high-influence users\")\n",
    "print(\"   Combining different centrality measures for maximum network coverage\\n\")\n",
    "# Select top users from different measures (diverse seed set)\n",
    "seed_set = set()\n",
    "seed_set.update(centrality_df.nlargest(3, 'pagerank')['user'])\n",
    "seed_set.update(centrality_df.nlargest(3, 'betweenness')['user'])\n",
    "seed_set.update(centrality_df.nlargest(3, 'eigenvector')['user'])\n",
    "seed_set.update(centrality_df.nlargest(3, 'closeness')['user'])\n",
    "\n",
    "seed_df = centrality_df[centrality_df['user'].isin(seed_set)].copy()\n",
    "seed_df = seed_df.sort_values('composite_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Network Visualization with Centrality Highlighting\n",
    "\n",
    "Let's visualize the network structure with nodes colored and sized by their centrality scores. This helps us see the network topology and identify influential users visually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"ðŸ“Š Creating network visualization...\")\n",
    "\n",
    "# Sample or subgraph if network is too large\n",
    "if G.number_of_nodes() > 500:\n",
    "    print(f\"   Network has {G.number_of_nodes()} nodes. Creating visualization with top influential users and their neighborhoods...\")\n",
    "    top_users = set(centrality_df.nlargest(1, 'composite_score')['user'])\n",
    "    neighborhood = set(top_users)\n",
    "    for user in top_users:\n",
    "        neighborhood.update(list(G.neighbors(user)))\n",
    "    G_viz = G.subgraph(neighborhood).copy()\n",
    "    print(f\"   Visualization subgraph: {G_viz.number_of_nodes()} nodes, {G_viz.number_of_edges()} edges\")\n",
    "else:\n",
    "    G_viz = G\n",
    "    print(f\"   Visualizing full network: {G_viz.number_of_nodes()} nodes\")\n",
    "\n",
    "print(\"   Computing network layout (this may take a moment)...\")\n",
    "pos = nx.spring_layout(G_viz, k=1, iterations=50, seed=42)\n",
    "\n",
    "# Create subplots (3 rows x 2 columns) for 5 measures, leave last cell empty\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=('Network by Degree Centrality', 'Network by Betweenness Centrality',\n",
    "                    'Network by PageRank', 'Network by Eigenvector Centrality',\n",
    "                    'Network by Katz Centrality', ''),\n",
    "    specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"scatter\"}, None]]\n",
    ")\n",
    "\n",
    "def create_network_plot(G_sub, pos_sub, centrality_dict, color_scale='Viridis'):\n",
    "    \"\"\"Create a network plot with nodes colored by centrality.\"\"\"\n",
    "    x_nodes = [pos_sub[node][0] for node in G_sub.nodes()]\n",
    "    y_nodes = [pos_sub[node][1] for node in G_sub.nodes()]\n",
    "    centrality_values = [centrality_dict.get(node, 0) for node in G_sub.nodes()]\n",
    "\n",
    "    edge_x, edge_y = [], []\n",
    "    for edge in G_sub.edges():\n",
    "        x0, y0 = pos_sub[edge[0]]\n",
    "        x1, y1 = pos_sub[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=0.5, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines',\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    node_trace = go.Scatter(\n",
    "        x=x_nodes, y=y_nodes,\n",
    "        mode='markers+text',\n",
    "        marker=dict(\n",
    "            size=[max(5, v * 500) if max(centrality_values) > 0 else 5 for v in centrality_values],\n",
    "            color=centrality_values,\n",
    "            colorscale=color_scale,\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Centrality\", len=0.3),\n",
    "            line=dict(width=0.5, color='white')\n",
    "        ),\n",
    "        text=[node if centrality_dict.get(node, 0) > np.percentile(centrality_values, 95) else '' \n",
    "              for node in G_sub.nodes()],\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(size=8),\n",
    "        hovertext=[f\"{node_to_name.get(node, node)}<br>Centrality: {centrality_dict.get(node, 0):.6f}\" \n",
    "                   for node in G_sub.nodes()],\n",
    "        hoverinfo='text',\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    return edge_trace, node_trace\n",
    "\n",
    "# Centrality measures and colors\n",
    "measures_viz = [\n",
    "    ('degree', 'Blues'),\n",
    "    ('betweenness', 'Reds'),\n",
    "    ('pagerank', 'Greens'),\n",
    "    ('eigenvector', 'Purples'),\n",
    "    ('katz', 'Oranges')\n",
    "]\n",
    "\n",
    "# Add traces to subplots\n",
    "for idx, (measure, color) in enumerate(measures_viz):\n",
    "    row = (idx // 2) + 1\n",
    "    col = (idx % 2) + 1\n",
    "    centrality_dict = dict(zip(centrality_df['user'], centrality_df[measure]))\n",
    "    edge_trace, node_trace = create_network_plot(G_viz, pos, centrality_dict, color)\n",
    "    fig.add_trace(edge_trace, row=row, col=col)\n",
    "    fig.add_trace(node_trace, row=row, col=col)\n",
    "\n",
    "# Remove axes\n",
    "for i in range(1, 4):\n",
    "    for j in range(1, 3):\n",
    "        fig.update_xaxes(showgrid=False, zeroline=False, showticklabels=False, row=i, col=j)\n",
    "        fig.update_yaxes(showgrid=False, zeroline=False, showticklabels=False, row=i, col=j)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1400,\n",
    "    showlegend=False,\n",
    "    title_text=\"Network Visualization by Different Centrality Measures<br><sub>Node size and color represent centrality score</sub>\",\n",
    "    title_x=0.5\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nâœ… Network visualization complete!\")\n",
    "print(\"   Larger, brighter nodes indicate higher centrality scores\")\n",
    "print(\"   Labels show only the top 5% most central nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final summary and insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š CENTRALITY ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Network Overview:\")\n",
    "print(f\"   Total Users: {G.number_of_nodes():,}\")\n",
    "print(f\"   Total Connections: {G.number_of_edges():,}\")\n",
    "print(f\"   Average Connections per User: {2*G.number_of_edges()/G.number_of_nodes():.2f}\")\n",
    "print(f\"   Network Density: {nx.density(G):.6f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Centrality Measure Insights:\")\n",
    "print(f\"   - Degree Centrality Range: [{centrality_df['degree'].min():.6f}, {centrality_df['degree'].max():.6f}]\")\n",
    "print(f\"   - Betweenness Centrality Range: [{centrality_df['betweenness'].min():.6f}, {centrality_df['betweenness'].max():.6f}]\")\n",
    "print(f\"   - PageRank Range: [{centrality_df['pagerank'].min():.6f}, {centrality_df['pagerank'].max():.6f}]\")\n",
    "print(f\"   - Katz Centrality Range: [{centrality_df['katz'].min():.6f}, {centrality_df['katz'].max():.6f}]\")\n",
    "\n",
    "# Key insights\n",
    "print(f\"\\nðŸ’¡ Key Insights:\")\n",
    "print(f\"   1. Network Structure:\")\n",
    "print(f\"      - {'Scale-free' if centrality_df['degree'].std() > centrality_df['degree'].mean() else 'More uniform'} degree distribution\")\n",
    "print(f\"      - Average clustering: {nx.average_clustering(G):.4f} (higher = more tight-knit communities)\")\n",
    "print(f\"      - Network diameter: {nx.diameter(G)} (maximum distance between any two users)\")\n",
    "\n",
    "print(f\"\\n   2. Influence Distribution:\")\n",
    "high_influence = len(centrality_df[centrality_df['composite_score'] > centrality_df['composite_score'].quantile(0.9)])\n",
    "print(f\"      - Top 10% influential users: {high_influence} users\")\n",
    "print(f\"      - These users control {100*high_influence/G.number_of_nodes():.2f}% of network influence\")\n",
    "\n",
    "print(f\"\\n   3. Centrality Correlations:\")\n",
    "corr_pagerank_degree = centrality_df['pagerank'].corr(centrality_df['degree'])\n",
    "corr_betweenness_eigen = centrality_df['betweenness'].corr(centrality_df['eigenvector'])\n",
    "print(f\"      - PageRank vs Degree: {corr_pagerank_degree:.3f} (high correlation = popular users are influential)\")\n",
    "print(f\"      - Betweenness vs Eigenvector: {corr_betweenness_eigen:.3f} (low correlation = different influence types)\")\n",
    "corr_katz_pagerank = centrality_df['katz'].corr(centrality_df['pagerank'])\n",
    "print(f\"      - Katz vs PageRank: {corr_katz_pagerank:.3f} (similar algorithms, different implementations)\")\n",
    "\n",
    "print(f\"\\n   4. Practical Applications:\")\n",
    "print(f\"      - Marketing: Target top {len(marketing_targets)} users by PageRank for maximum reach\")\n",
    "print(f\"      - Information Spread: Use top {len(info_spreaders)} users by Closeness for fast dissemination\")\n",
    "print(f\"      - Community Bridges: Leverage top {len(bridge_users)} users by Betweenness for cross-group communication\")\n",
    "\n",
    "# Create final summary dashboard\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Top 20 Influential Users', 'Centrality Score Distributions'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"histogram\"}]]\n",
    ")\n",
    "\n",
    "# Top 20 users\n",
    "top_20 = centrality_df.nlargest(20, 'composite_score')\n",
    "fig.add_trace(\n",
    "    go.Bar(x=top_20['display_name'].map(lambda x: node_to_name.get(x, x)), y=top_20['composite_score'],\n",
    "           marker_color='#FF6B6B', text=top_20['composite_score'].round(4),\n",
    "           textposition='outside', name='Composite Score'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Score distributions\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=centrality_df['composite_score'], nbinsx=30,\n",
    "                marker_color='#4ECDC4', name='Composite Score Distribution'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(tickangle=-45, row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Composite Influence Score\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Number of Direct Connections\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Number of Top 100 Users\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    showlegend=False,\n",
    "    title_text=\"Centrality Analysis Summary Dashboard\",\n",
    "    title_x=0.5\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nâœ… Analysis complete! Review the visualizations above for detailed insights.\")\n",
    "print(\"\\nðŸŽ“ Key Takeaways:\")\n",
    "print(\"   - Different centrality measures reveal different aspects of influence\")\n",
    "print(\"   - High-degree users (celebrities) are not always the most influential\")\n",
    "print(\"   - Bridge users (high betweenness) are crucial for information flow\")\n",
    "print(\"   - Combining multiple measures provides a more complete picture\")\n",
    "print(\"   - Understanding influence types helps in targeted marketing and communication strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
