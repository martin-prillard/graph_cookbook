{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978958f8",
   "metadata": {
    "id": "978958f8"
   },
   "source": [
    "# Malware Classification with Graph Embeddings\n",
    "\n",
    "This notebook builds an end-to-end workflow to detect malware from function-call graphs. We rely on the MalNet Tiny dataset distributed via PyTorch Geometric, transform the graphs into vector representations, explore their structure, and finally train both classical ML and GNN models for classification. The dataset is pulled automatically from the [PyTorch Geometric documentation](https://pytorch-geometric.readthedocs.io/en/2.4.0/generated/torch_geometric.datasets.MalNetTiny.html) so no manual downloads are required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae99ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed. Then restart the jupyter kernel\n",
    "!uv pip install --no-cache-dir --force-reinstall joblib==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IaIrgl-tu9St",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IaIrgl-tu9St",
    "outputId": "9e009821-1abc-4833-ac8e-9e93d9becdc3"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from karateclub import Graph2Vec\n",
    "from umap.umap_ import UMAP\n",
    "import networkx as nx\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "from pycaret.classification import (\n",
    "    setup, compare_models, finalize_model,\n",
    "    tune_model, evaluate_model, save_model, load_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5450a3",
   "metadata": {
    "id": "0f5450a3"
   },
   "source": [
    "## 1. Load MalNet Tiny Graphs\n",
    "Use the `MalNetTiny` dataset helper to download the graphs automatically, and keep a balanced subset of 200 graphs per class for faster experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d61ea2-5b66-402d-8fa6-67e7766050fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_GRAPHS = '../../data/malnet-graphs-tiny'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb442f-803d-428c-9ee7-06de8ad44734",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "CLASSES = ['addisplay', 'adware', 'benign', 'downloader', 'trojan']\n",
    "MAX_GRAPHS_BY_CLASSE = 200\n",
    "\n",
    "targets = []\n",
    "graphs = []\n",
    "\n",
    "for classe in CLASSES:\n",
    "    files = Path(PATH_GRAPHS + '/' + classe).glob('*.edgelist')\n",
    "    for i, file in tqdm(enumerate(files)):\n",
    "        if i >= MAX_GRAPHS_BY_CLASSE:\n",
    "            break\n",
    "        targets.append(classe)\n",
    "        G = nx.read_edgelist(file)\n",
    "        G = nx.convert_node_labels_to_integers(G, label_attribute='old_label')\n",
    "        graphs.append(G)\n",
    "\n",
    "f\"{len(graphs)} graphes chargés ({dict(Counter(targets))})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda7dc9c",
   "metadata": {
    "id": "fda7dc9c"
   },
   "source": [
    "## 2. Graph Embedding\n",
    "Learn Graph2Vec representations that turn each graph into a dense vector suitable for downstream visualization and classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369dc814",
   "metadata": {
    "id": "369dc814"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "N_DIMENSIONS = 2\n",
    "\n",
    "graph2vec = Graph2Vec(dimensions=N_DIMENSIONS)\n",
    "graph2vec.fit(graphs)\n",
    "embeddings = graph2vec.get_embedding()\n",
    "print(embeddings.shape)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6b2f68",
   "metadata": {
    "id": "db6b2f68"
   },
   "source": [
    "**Plot the embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hl9z4JL9lh3W",
   "metadata": {
    "id": "hl9z4JL9lh3W"
   },
   "source": [
    "The interactive scatterplot helps verify whether Graph2Vec separates the malware families.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e178619",
   "metadata": {
    "id": "9e178619"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=embeddings[:, 0], y=embeddings[:, 1], color=targets)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952eeb1",
   "metadata": {
    "id": "2952eeb1"
   },
   "source": [
    "## 3. Dimensionality Reduction\n",
    "Use UMAP to reduce the high-dimensional embeddings to 2D and 3D views that make cluster structures easier to inspect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ee5c8",
   "metadata": {
    "id": "685ee5c8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "N_DIMENSIONS = 512\n",
    "\n",
    "graph2vec = Graph2Vec(dimensions=N_DIMENSIONS, seed=42)\n",
    "graph2vec.fit(graphs)\n",
    "embeddings = graph2vec.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339dc33e",
   "metadata": {
    "id": "339dc33e"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(embeddings)\n",
    "df['target'] = targets\n",
    "df.to_csv('../../data/malware_emb.csv', index=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e992e7a",
   "metadata": {
    "id": "5e992e7a"
   },
   "source": [
    "**Project embeddings down to 2 dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30980523",
   "metadata": {
    "id": "30980523"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "proj_2d = UMAP(n_components=2, init='random', random_state=0).fit_transform(embeddings)\n",
    "proj_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae71c7",
   "metadata": {
    "id": "5dae71c7"
   },
   "source": [
    "**Plot the 2D embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2gTfpKI1XJ",
   "metadata": {
    "id": "cb2gTfpKI1XJ"
   },
   "outputs": [],
   "source": [
    "fig_2d = px.scatter(\n",
    "    proj_2d, x=0, y=1,\n",
    "    color=targets\n",
    ")\n",
    "fig_2d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b7899b",
   "metadata": {
    "id": "b3b7899b"
   },
   "source": [
    "**Project embeddings down to 3 dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c21171",
   "metadata": {
    "id": "e0c21171"
   },
   "outputs": [],
   "source": [
    "proj_3d = UMAP(n_components=3, init='random', random_state=0).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b800e87",
   "metadata": {
    "id": "4b800e87"
   },
   "source": [
    "**Plot the 3D embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135dc66f",
   "metadata": {
    "id": "135dc66f"
   },
   "outputs": [],
   "source": [
    "fig_3d = px.scatter_3d(\n",
    "    proj_3d, x=0, y=1, z=2,\n",
    "    color=targets,\n",
    "    height=700\n",
    ")\n",
    "fig_3d.update_traces(marker_size=5)\n",
    "fig_3d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25601f4e-a7cb-4793-9a88-b75fe8b2e832",
   "metadata": {},
   "source": [
    "#### Versus bad initial approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72744b04-6f81-4f3c-babf-5ba19668d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "N_DIMENSIONS = 3\n",
    "\n",
    "graph2vec = Graph2Vec(dimensions=N_DIMENSIONS)\n",
    "graph2vec.fit(graphs)\n",
    "embeddings = graph2vec.get_embedding()\n",
    "print(embeddings.shape)\n",
    "\n",
    "fig_3d = px.scatter_3d(\n",
    "    embeddings, x=0, y=1, z=2,\n",
    "    color=targets,\n",
    "    height=700\n",
    ")\n",
    "fig_3d.update_traces(marker_size=5)\n",
    "fig_3d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55510dde",
   "metadata": {
    "id": "55510dde"
   },
   "source": [
    "## 4. Classical Classification\n",
    "Each graph is annotated with a malware family (or the benign label), so we can train a supervised classifier that predicts one of the five categories automatically. We rely on PyCaret to quickly compare algorithms, tune the best one, and persist the winning model for later inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d7f35e",
   "metadata": {
    "id": "80d7f35e"
   },
   "source": [
    "**Load the saved embeddings**\n",
    "\n",
    "Read the CSV file that stores the Graph2Vec representations alongside their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47583789",
   "metadata": {
    "id": "47583789"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/malware_emb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2e2c4",
   "metadata": {
    "id": "27f2e2c4"
   },
   "source": [
    "**Initialize PyCaret**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8536fe",
   "metadata": {
    "id": "ca8536fe"
   },
   "outputs": [],
   "source": [
    "setup(df, target=\"target\", fold=3, html=True) # 0.8658"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c576fac",
   "metadata": {
    "id": "2c576fac"
   },
   "source": [
    "**Compare models**\n",
    "PyCaret benchmarks a wide range of classifiers so we can pick the one that offers the best accuracy for the selected embedding dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba97038-432d-49ba-bd19-ffe9f5267ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add scikit-learn==1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = compare_models(exclude=['lightgbm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2d5be",
   "metadata": {
    "id": "dbe2d5be"
   },
   "source": [
    "**Tune the best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb658b",
   "metadata": {
    "id": "39eb658b"
   },
   "outputs": [],
   "source": [
    "best_model_tuned = tune_model(best_model, search_library='optuna')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ee6ae",
   "metadata": {
    "id": "fb5ee6ae"
   },
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a2a48",
   "metadata": {
    "id": "df1a2a48"
   },
   "outputs": [],
   "source": [
    "evaluate_model(best_model)\n",
    "# addisplay: 0, adware: 1, benign: 2, downloader: 3, trojan: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c1787",
   "metadata": {
    "id": "6e9c1787"
   },
   "source": [
    "**Save final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1e336",
   "metadata": {
    "id": "a4a1e336"
   },
   "outputs": [],
   "source": [
    "final_model = finalize_model(best_model)\n",
    "save_model(final_model, 'ml_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81052e1e",
   "metadata": {
    "id": "81052e1e"
   },
   "outputs": [],
   "source": [
    "best_model = load_model('ml_model')\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57789ca0",
   "metadata": {},
   "source": [
    "### Autogluon VS PyCaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install autogluon\n",
    "!uv sync --extra autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddcf5a5-9b55-4a83-a9c4-00f08a53ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor, TabularDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992fdbd",
   "metadata": {},
   "source": [
    "Split train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e39b5-e776-4f42-a294-78a05459fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, _, _ = train_test_split(df, targets, test_size=0.33, random_state=42)\n",
    "X_train.to_csv('train.csv', index=None)\n",
    "X_test.to_csv('test.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689efc62",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e44247-e9d6-459f-8217-4a75a6125605",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor(label=\"target\").fit(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799d050",
   "metadata": {},
   "source": [
    "Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965ca73-bee1-4d67-9373-dd9724a84b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TabularDataset(f'test.csv')\n",
    "\n",
    "y_pred = predictor.predict(test_data.drop(columns=['target']))\n",
    "y_pred[:10]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f9db4",
   "metadata": {},
   "source": [
    "Evaluation of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3866534-f5a8-441d-89d0-11c927ed3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e88761-9150-4ef4-9304-74908b3df9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i68ofqznPJ3o",
   "metadata": {
    "id": "i68ofqznPJ3o"
   },
   "source": [
    "## 5. GNN-Based Classification\n",
    "Explore an end-to-end neural approach by training a Graph Convolutional Network (GCN) on the raw graphs instead of relying on precomputed embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install torch and torch_geometric\n",
    "!uv sync --extra deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c042304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Dropout\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import BatchNorm, GCNConv, global_mean_pool\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "\n",
    "# hardware selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.version.cuda)\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.cuda.get_device_capability(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JdOejP9kPjSZ",
   "metadata": {
    "id": "JdOejP9kPjSZ"
   },
   "source": [
    "**Convert NetworkX graphs into PyG Data objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fW71glC0t6Rz",
   "metadata": {
    "id": "fW71glC0t6Rz"
   },
   "outputs": [],
   "source": [
    "# Convert NetworkX graphs into PyG Data objects, adding placeholder node features and labels.\n",
    "def convert_to_pyg(graphs, targets):\n",
    "    data_list = []\n",
    "    for i, graph in enumerate(graphs):\n",
    "        for node in graph.nodes():\n",
    "            graph.nodes[node]['x'] = [1.0]  # Constant node feature placeholder\n",
    "\n",
    "        data = from_networkx(graph)\n",
    "        data.y = torch.tensor([targets[i]], dtype=torch.long)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "class_mapping = {'addisplay': 0, 'adware': 1, 'benign': 2, 'downloader': 3, 'trojan': 4}\n",
    "encoded_targets = [class_mapping[label] for label in targets]\n",
    "\n",
    "data_list = convert_to_pyg(graphs, encoded_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qglUVAZUPa4J",
   "metadata": {
    "id": "qglUVAZUPa4J"
   },
   "source": [
    "**Create train/test splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z_2qLd2XulW_",
   "metadata": {
    "id": "Z_2qLd2XulW_"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train and test partitions\n",
    "train_data, test_data = train_test_split(\n",
    "    data_list,\n",
    "    test_size=int(len(data_list) * 0.3),\n",
    "    stratify=encoded_targets,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Build PyG dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ShA__SbTP7Sz",
   "metadata": {
    "id": "ShA__SbTP7Sz"
   },
   "source": [
    "**Define the GNN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CT5SVZ4L7LrE",
   "metadata": {
    "id": "CT5SVZ4L7LrE"
   },
   "outputs": [],
   "source": [
    "# Deeper GNN classifier with normalization and dropout regularization\n",
    "class ImprovedGNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        self.norms.append(BatchNorm(hidden_channels))\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18017f4",
   "metadata": {},
   "source": [
    "Load or create a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_previous_model = False\n",
    "\n",
    "if load_previous_model:\n",
    "    model = ImprovedGNNClassifier(\n",
    "        in_channels=checkpoint[\"in_channels\"],\n",
    "        hidden_channels=checkpoint[\"hidden_channels\"],\n",
    "        out_channels=checkpoint[\"out_channels\"],\n",
    "        num_layers=checkpoint[\"num_layers\"],\n",
    "        dropout=checkpoint[\"dropout\"],\n",
    "    ).to(device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.005,          # même LR qu’à l’entraînement initial\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "else:\n",
    "    in_channels = 1\n",
    "    hidden_channels = 256  # Increased hidden size for better capacity\n",
    "    out_channels = len(class_mapping)\n",
    "    num_layers = 8  # Stack more GCN layers\n",
    "    dropout = 0.5\n",
    "\n",
    "    model = ImprovedGNNClassifier(in_channels, hidden_channels, out_channels, num_layers, dropout)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    start_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / max(1, len(train_loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    preds, labels = [], []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch)\n",
    "        predictions = out.argmax(dim=1)\n",
    "        correct += (predictions == batch.y).sum().item()\n",
    "        preds.extend(predictions.cpu().tolist())\n",
    "        labels.extend(batch.y.cpu().tolist())\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return accuracy, preds, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Amy-HAmbhd1T",
   "metadata": {
    "id": "Amy-HAmbhd1T"
   },
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36c12a-cdbc-4ed3-8b07-c1e5ccb2d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, update your torch version\n",
    "# !uv pip uninstall torch\n",
    "# !uv pip install torch==2.10.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df8Bl47ZHe",
   "metadata": {
    "id": "32df8Bl47ZHe"
   },
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "num_epochs = 120\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    loss = train()\n",
    "    train_acc, _, _ = test(train_loader)\n",
    "    test_acc, _, _ = test(test_loader)\n",
    "    print(f\"Epoch {epoch:02d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    # Step the scheduler based on the latest loss\n",
    "    scheduler.step(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7cec76",
   "metadata": {},
   "source": [
    "A previous result:\n",
    "```\n",
    "Epoch 01, Loss: 1.0301, Train Acc: 0.5174, Test Acc: 0.5153\n",
    "Epoch 02, Loss: 0.8389, Train Acc: 0.6200, Test Acc: 0.6033\n",
    "Epoch 03, Loss: 0.8088, Train Acc: 0.5354, Test Acc: 0.5287\n",
    "Epoch 04, Loss: 0.7750, Train Acc: 0.5603, Test Acc: 0.5600\n",
    "Epoch 05, Loss: 0.7607, Train Acc: 0.6637, Test Acc: 0.6480\n",
    "Epoch 06, Loss: 0.7577, Train Acc: 0.6720, Test Acc: 0.6607\n",
    "Epoch 07, Loss: 0.7170, Train Acc: 0.6157, Test Acc: 0.5973\n",
    "Epoch 08, Loss: 0.6969, Train Acc: 0.6829, Test Acc: 0.6687\n",
    "Epoch 09, Loss: 0.6886, Train Acc: 0.6023, Test Acc: 0.5793\n",
    "Epoch 10, Loss: 0.6733, Train Acc: 0.6937, Test Acc: 0.6733\n",
    "Epoch 11, Loss: 0.6586, Train Acc: 0.7749, Test Acc: 0.7540\n",
    "Epoch 12, Loss: 0.6440, Train Acc: 0.7511, Test Acc: 0.7487\n",
    "Epoch 13, Loss: 0.6406, Train Acc: 0.5489, Test Acc: 0.5467\n",
    "Epoch 14, Loss: 0.6336, Train Acc: 0.7774, Test Acc: 0.7660\n",
    "Epoch 15, Loss: 0.6112, Train Acc: 0.6420, Test Acc: 0.6320\n",
    "Epoch 16, Loss: 0.6018, Train Acc: 0.6291, Test Acc: 0.6140\n",
    "Epoch 17, Loss: 0.6030, Train Acc: 0.7763, Test Acc: 0.7747\n",
    "Epoch 18, Loss: 0.5758, Train Acc: 0.7857, Test Acc: 0.7813\n",
    "Epoch 19, Loss: 0.5853, Train Acc: 0.7374, Test Acc: 0.7353\n",
    "Epoch 20, Loss: 0.5772, Train Acc: 0.5374, Test Acc: 0.5240\n",
    "Epoch 21, Loss: 0.5930, Train Acc: 0.7754, Test Acc: 0.7607\n",
    "Epoch 22, Loss: 0.5513, Train Acc: 0.7809, Test Acc: 0.7760\n",
    "Epoch 23, Loss: 0.5405, Train Acc: 0.8166, Test Acc: 0.8053\n",
    "Epoch 24, Loss: 0.5582, Train Acc: 0.7626, Test Acc: 0.7413\n",
    "Epoch 25, Loss: 0.5531, Train Acc: 0.8231, Test Acc: 0.8080\n",
    "Epoch 26, Loss: 0.5529, Train Acc: 0.7766, Test Acc: 0.7593\n",
    "Epoch 27, Loss: 0.5257, Train Acc: 0.7771, Test Acc: 0.7467\n",
    "Epoch 28, Loss: 0.5465, Train Acc: 0.7631, Test Acc: 0.7467\n",
    "Epoch 29, Loss: 0.5231, Train Acc: 0.8194, Test Acc: 0.7927\n",
    "Epoch 30, Loss: 0.5167, Train Acc: 0.8326, Test Acc: 0.8127\n",
    "Epoch 31, Loss: 0.5167, Train Acc: 0.7423, Test Acc: 0.7253\n",
    "Epoch 32, Loss: 0.5202, Train Acc: 0.8291, Test Acc: 0.8113\n",
    "Epoch 33, Loss: 0.5230, Train Acc: 0.7911, Test Acc: 0.7700\n",
    "Epoch 34, Loss: 0.4981, Train Acc: 0.8377, Test Acc: 0.8200\n",
    "Epoch 35, Loss: 0.5222, Train Acc: 0.7386, Test Acc: 0.7267\n",
    "Epoch 36, Loss: 0.5015, Train Acc: 0.7903, Test Acc: 0.7680\n",
    "Epoch 37, Loss: 0.4928, Train Acc: 0.8089, Test Acc: 0.7913\n",
    "Epoch 38, Loss: 0.5122, Train Acc: 0.6820, Test Acc: 0.6653\n",
    "Epoch 39, Loss: 0.4862, Train Acc: 0.8469, Test Acc: 0.8327\n",
    "Epoch 40, Loss: 0.4738, Train Acc: 0.8520, Test Acc: 0.8373\n",
    "Epoch 41, Loss: 0.4997, Train Acc: 0.8377, Test Acc: 0.8227\n",
    "Epoch 42, Loss: 0.5066, Train Acc: 0.7291, Test Acc: 0.7140\n",
    "Epoch 43, Loss: 0.4735, Train Acc: 0.7591, Test Acc: 0.7507\n",
    "Epoch 44, Loss: 0.4872, Train Acc: 0.8377, Test Acc: 0.8220\n",
    "Epoch 45, Loss: 0.4716, Train Acc: 0.8629, Test Acc: 0.8360\n",
    "Epoch 46, Loss: 0.4819, Train Acc: 0.8617, Test Acc: 0.8280\n",
    "Epoch 47, Loss: 0.4629, Train Acc: 0.8363, Test Acc: 0.8140\n",
    "Epoch 48, Loss: 0.4457, Train Acc: 0.8720, Test Acc: 0.8433\n",
    "Epoch 49, Loss: 0.4503, Train Acc: 0.7811, Test Acc: 0.7653\n",
    "Epoch 50, Loss: 0.4520, Train Acc: 0.8346, Test Acc: 0.8087\n",
    "Epoch 51, Loss: 0.4787, Train Acc: 0.8520, Test Acc: 0.8240\n",
    "Epoch 52, Loss: 0.4457, Train Acc: 0.8523, Test Acc: 0.8273\n",
    "Epoch 53, Loss: 0.4662, Train Acc: 0.8520, Test Acc: 0.8187\n",
    "Epoch 54, Loss: 0.4388, Train Acc: 0.8666, Test Acc: 0.8487\n",
    "Epoch 55, Loss: 0.4587, Train Acc: 0.8554, Test Acc: 0.8260\n",
    "Epoch 56, Loss: 0.4575, Train Acc: 0.8603, Test Acc: 0.8387\n",
    "Epoch 57, Loss: 0.4202, Train Acc: 0.8709, Test Acc: 0.8400\n",
    "Epoch 58, Loss: 0.4205, Train Acc: 0.8751, Test Acc: 0.8367\n",
    "Epoch 59, Loss: 0.4349, Train Acc: 0.8697, Test Acc: 0.8520\n",
    "Epoch 60, Loss: 0.4291, Train Acc: 0.8434, Test Acc: 0.8240\n",
    "Epoch 61, Loss: 0.4300, Train Acc: 0.7560, Test Acc: 0.7327\n",
    "Epoch 62, Loss: 0.4213, Train Acc: 0.8620, Test Acc: 0.8460\n",
    "Epoch 63, Loss: 0.4200, Train Acc: 0.8737, Test Acc: 0.8480\n",
    "Epoch 64, Loss: 0.4226, Train Acc: 0.8894, Test Acc: 0.8633\n",
    "Epoch 65, Loss: 0.4125, Train Acc: 0.8634, Test Acc: 0.8373\n",
    "Epoch 66, Loss: 0.4110, Train Acc: 0.8686, Test Acc: 0.8380\n",
    "Epoch 67, Loss: 0.4154, Train Acc: 0.8434, Test Acc: 0.8127\n",
    "Epoch 68, Loss: 0.3913, Train Acc: 0.8809, Test Acc: 0.8540\n",
    "Epoch 69, Loss: 0.3951, Train Acc: 0.8783, Test Acc: 0.8500\n",
    "Epoch 70, Loss: 0.3896, Train Acc: 0.8771, Test Acc: 0.8493\n",
    "Epoch 71, Loss: 0.3807, Train Acc: 0.8671, Test Acc: 0.8500\n",
    "Epoch 72, Loss: 0.3976, Train Acc: 0.8529, Test Acc: 0.8360\n",
    "Epoch 73, Loss: 0.3960, Train Acc: 0.8763, Test Acc: 0.8400\n",
    "Epoch 74, Loss: 0.4049, Train Acc: 0.8714, Test Acc: 0.8320\n",
    "Epoch 75, Loss: 0.3970, Train Acc: 0.8809, Test Acc: 0.8573\n",
    "Epoch 76, Loss: 0.4003, Train Acc: 0.8629, Test Acc: 0.8327\n",
    "Epoch 77, Loss: 0.3927, Train Acc: 0.8874, Test Acc: 0.8667\n",
    "Epoch 78, Loss: 0.3541, Train Acc: 0.9040, Test Acc: 0.8813\n",
    "Epoch 79, Loss: 0.3515, Train Acc: 0.8814, Test Acc: 0.8373\n",
    "Epoch 80, Loss: 0.3496, Train Acc: 0.8977, Test Acc: 0.8693\n",
    "Epoch 81, Loss: 0.3393, Train Acc: 0.9031, Test Acc: 0.8740\n",
    "Epoch 82, Loss: 0.3398, Train Acc: 0.8734, Test Acc: 0.8327\n",
    "Epoch 83, Loss: 0.3437, Train Acc: 0.9126, Test Acc: 0.8833\n",
    "Epoch 84, Loss: 0.3306, Train Acc: 0.8657, Test Acc: 0.8313\n",
    "Epoch 85, Loss: 0.3125, Train Acc: 0.8997, Test Acc: 0.8640\n",
    "Epoch 86, Loss: 0.3107, Train Acc: 0.9089, Test Acc: 0.8627\n",
    "Epoch 87, Loss: 0.3271, Train Acc: 0.9103, Test Acc: 0.8780\n",
    "Epoch 88, Loss: 0.3325, Train Acc: 0.8997, Test Acc: 0.8673\n",
    "Epoch 89, Loss: 0.3325, Train Acc: 0.9040, Test Acc: 0.8860\n",
    "Epoch 90, Loss: 0.3404, Train Acc: 0.9174, Test Acc: 0.8840\n",
    "Epoch 91, Loss: 0.3225, Train Acc: 0.9149, Test Acc: 0.8767\n",
    "Epoch 92, Loss: 0.3150, Train Acc: 0.9103, Test Acc: 0.8640\n",
    "Epoch 93, Loss: 0.3097, Train Acc: 0.9189, Test Acc: 0.8753\n",
    "Epoch 94, Loss: 0.3011, Train Acc: 0.9186, Test Acc: 0.8827\n",
    "Epoch 95, Loss: 0.2842, Train Acc: 0.9206, Test Acc: 0.8773\n",
    "Epoch 96, Loss: 0.3029, Train Acc: 0.9254, Test Acc: 0.8900\n",
    "Epoch 97, Loss: 0.2822, Train Acc: 0.9226, Test Acc: 0.8867\n",
    "Epoch 98, Loss: 0.2841, Train Acc: 0.9197, Test Acc: 0.8787\n",
    "Epoch 99, Loss: 0.2867, Train Acc: 0.9209, Test Acc: 0.8833\n",
    "Epoch 100, Loss: 0.2917, Train Acc: 0.9266, Test Acc: 0.8840\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1697be0",
   "metadata": {},
   "source": [
    "**Display results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1200ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport final\n",
    "_, all_preds, all_labels = test(test_loader)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=list(class_mapping.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a1714e",
   "metadata": {},
   "source": [
    "**Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"epoch\": epoch,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    # hyperparams modèle\n",
    "    \"in_channels\": in_channels,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"out_channels\": out_channels,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"dropout\": dropout,\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \"improved_gnn_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181b009-5e65-449c-af86-5c52686bf410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
